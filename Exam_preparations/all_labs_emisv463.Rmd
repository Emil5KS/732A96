---
title: "all_labs_emil"
author: "Emil K Svensson"
date: "16 October 2017"
output: pdf_document
---

# Lab 1

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')
```


```{r, echo=FALSE}
library(bnlearn)
library(gRain)
library(Rgraphviz)

## install.packages("bnlearn")
## source("https://bioconductor.org/biocLite.R")
## biocLite("RBGL")
## install.packages("gRain")
## source("https://bioconductor.org/biocLite.R")
## biocLite("Rgraphviz")
```

# Assignment 1

```{r, echo=FALSE, cache=TRUE}
true_net <- empty.graph(names(learning.test))
modelstring(true_net) = "[A][C][F][B|A][D|A:C][E|B:F]"
```

```{r}
graph_equality <- function(g1, g2) {
    all.equal(cpdag(g1), cpdag(g2)) == TRUE
}

print("No restarts")
sapply(1:5, function(i) {
    g1 <- hc(alarm, score="bde", iss=1, restart=0)
    g2 <- hc(alarm, score="bde", iss=1, restart=0)
    graph_equality(g1, g2)
})

print("Restarts with equivalent seed")
sapply(1:5, function(i) {
    set.seed(i)
    g1 <- hc(alarm, score="bde", iss=1, restart=10)
    set.seed(i)
    g2 <- hc(alarm, score="bde", iss=1, restart=10)
    graph_equality(g1, g2)
})

print("Restarts with distinct seed")
sapply(1:5, function(i) {
    set.seed(i)
    g1 <- hc(alarm, score="bde", iss=1, restart=10)
    set.seed(2 * i)
    g2 <- hc(alarm, score="bde", iss=1, restart=10)
    graph_equality(g1, g2)
})
```

The hill climbing algorithm is completely deterministic so unless we do random restarts the result is the same. The random restarts means that we begin the algorithm with different initial graph structures which may possibly end up in different local optima.

\newpage

# Assignment 2

```{r, cache=TRUE}
data <- asia

g1 <- hc(data, score="bde", iss=1, restart=10)
g10 <- hc(data, score="bde", iss=10, restart=10)
g100 <- hc(data, score="bde", iss=100, restart=10)
g1000 <- hc(data, score="bde", iss=1000, restart=10)

bnlearn::score(g1, data, type="bde")
bnlearn::score(g10, data, type="bde")
bnlearn::score(g100, data, type="bde")
bnlearn::score(g1000, data, type="bde")
```

In the BDe (\textit{Bayesian Dirichlet equivalent uniform}) score we assume a priori that the probabilities follow a Dirichlet($\alpha$) where $\alpha$ is the \textit{imaginary sample size} (iss) and the posterior

\begin{equation*}
P(A | Data) = \frac{iss}{n + iss} P_{\text{prior}}(A) + \frac{n}{n + iss} P_{\text{empirical}}(A),
\end{equation*}

where $n$ is the number of observations in the data. This means that the iss controls how certain we are a priori of the probabilities indicating that having a high iss means the data have less influence on the posterior distribution. Since the Dirichlet is chosen such that it is uniform a higher iss result in more edges in the graph, therefore iss can be seen as a regularization term. A low value entails a sparse graph, i.e. less arcs/dependencies, inferred from data assuming the true distribution is represented by a sparse graph.

This can clearly be shown by the plot below. Given that there are 8 nodes in the graph we would expect the average branching factor to be around $7 / 2 = 3.5$  with a uniform prior and we get $3$ with a imaginary sample size of 1000 which is close. Bayesian networks have certain constraints that prevent edges from being added arbitrarly.

```{r, echo=FALSE}
oldpar <- par(mfrow=c(2, 1))
graphviz.plot(g1, main="iss=1")
graphviz.plot(g1000, main="iss=1000")
par(oldpar)
```

\newpage

# Assignment 3

```{r}
data <- asia
graph <- hc(data, score="bde", iss=1, restart=10)
bayes_net <- bn.fit(graph, data, method="bayes", iss=1)
junction_tree <- compile(as.grain(bayes_net))
```

```{r, echo=FALSE}
plot(junction_tree)
```

## Aproximate inference
```{r}
#Aproximate inference for B with no evidence 
dist <- cpdist(fitted=bayes_net, nodes=c("B"), evidence=TRUE)
prop.table(table(dist))

# aprox inf. for B w. no evidence but another method
dist <- cpdist(fitted=bayes_net, nodes=c("B"), evidence=TRUE, method="lw")
prop.table(table(dist))

# aprox inf. for L and T w/o any evidence (observations)
dist <- cpdist(fitted=bayes_net, nodes=c("L", "T"), evidence=TRUE)
prop.table(table(dist))

# aprox inf. for L and T with having node E observed as yes
dist <- cpdist(fitted=bayes_net, nodes=c("L", "T"), evidence=(E == "yes"))
prop.table(table(dist))

# aprox inf. for D given that we have observed B as yes and E as yes. 
# We have also increased the number of samples generated.
dist <- cpdist(bayes_net, nodes="D", evidence=(B=="yes") & (E=="yes"), n=10^6)
prop.table(table(dist))

dist <- cpdist(bayes_net, nodes="D", evidence=(B=="yes") & (E=="no"))
prop.table(table(dist))

dist <- cpdist(bayes_net, nodes="D", evidence=(B=="no") & (E=="yes"), n=10^6)
prop.table(table(dist))

dist <- cpdist(bayes_net, nodes="D", evidence=(B=="no") & (E=="no"))
prop.table(table(dist))
```

## Exact inference

```{r}
## Exact inference
querygrain(junction_tree, nodes=c("B"), type="marginal")
querygrain(junction_tree, nodes=c("B"), type="joint")
#querygrain(junction_tree, nodes=c("B"), type="conditional")


## Exact inference
querygrain(junction_tree, nodes=c("L", "T"), type="marginal")
querygrain(junction_tree, nodes=c("L", "T"), type="joint")
querygrain(junction_tree, nodes=c("L", "T"), type="conditional")



## Exact Inference
querygrain(setEvidence(junction_tree, nodes="E", states="yes"),
           nodes=c("L", "T"), type="joint")

querygrain(setEvidence(junction_tree, nodes="E", states="yes"),
           nodes=c("L", "T"), type="marginal")

querygrain(setEvidence(junction_tree, nodes="E", states="yes"),
           nodes=c("L", "T"), type="conditional")

## Exact Inference
querygrain(junction_tree, nodes=c("D", "B", "E"), type="conditional")

```


In the approximate inference methods we use simulated data to infer the queries and the samples are random so the inference will also be random. When the query includes observed nodes it seems that the approximate inference algorithm require more samples to be reasonably accurate than for queries without any observed nodes. This is probabily due to the sampling process is sampling from the full joint distribution rather than the conditional distribution. This means that not all samples fullfil the conditional so the actual sample size is far less than specified. This is evident from the following code block.

```{r}
## No observed nodes
number_of_samples <- 10^6
dist <- cpdist(bayes_net, nodes="D", evidence=TRUE, n=number_of_samples)
number_of_actual_samples <- nrow(dist)
sprintf("Number of samples to generate: %i, Number of samples returned: %i",
        number_of_samples, number_of_actual_samples)

## Two observed nodes
number_of_samples <- 10^6
dist <- cpdist(bayes_net, nodes="D", evidence=(B=="no") & (E=="yes"), n=number_of_samples)
number_of_actual_samples <- nrow(dist)
sprintf("Number of samples to generate: %i, Number of samples returned: %i",
        number_of_samples, number_of_actual_samples)
```

\newpage

# Assignment 4

```{r, cache=TRUE}
n <- 10000
rgraphs <- unique(random.graph(nodes=c("1", "2", "3", "4", "5"), num=n,
                               method="ic-dag", burn.in=500, every=10))
length(unique(lapply(rgraphs, cpdag))) / length(rgraphs)
```

From the evidence we would reduce the search space by approximately 45\% which is a lot relative to how many DAGs there are as the number of nodes increases. However, I imagine it is more difficult to work with the essential graphs due to containing both directed and undirected arcs. Depending on the increased complexity of the algorithm working in the essential graph space could potentially be appropriate for structure learning.



# Lab 2

# Question 1 

```{r}
hidden_states <- paste("z",1:10,sep = "")
observed_states <- paste("x",1:10,sep = "")
start <- rep(0.1,10)

#Just to see the structure
#initHMM(states, observed_states)

# The transition probabilities, since we only can move 
trans <- matrix(0,ncol = 10, nrow = 10)
diag(trans) <- 0.5
diag(trans[,-1]) <- 0.5
trans[10,1] <- 0.5 
#trans[1,10] <- 0.5

# Making sure the probabilities sum to 1 in each row
#apply(trans, MARGIN = 1, FUN = sum)
#rep(1:5, each = 2)

# The Emission probabilities are our uncertaintiec in the position of the robot. Basicly the observations that we cannot observe. 

emission <- 
  diag(1/5, 10)[, c(3:10, 1:2)] +
  diag(1/5, 10)[, c(2:10, 1)] +
  diag(1/5, 10) + 
  diag(1/5, 10)[, c(10, 1:9)] + 
  diag(1/5, 10)[, c(9:10, 1:8)]

colnames(emission) <- paste("x",1:10,sep = "")
rownames(emission) <- paste("z",1:10,sep = "")

rownames(trans) <- paste("z",1:10,sep = "")
colnames(trans) <- paste("z",1:10,sep = "")

```
\newpage

```{r}


# The different places the robot can be in
observed_states

# The Hidden States  
hidden_states

# Where they start
start

# Transition matrix is the probabilities the robot will move
trans

# Emission matrix states the uncertainties we have about the robots position
emission

```
With everything defined we can initialize the HMM-robot. 

```{r}
robot <- initHMM(hidden_states,observed_states,start,trans,emission)

```

# Question 2

The function below is built for answering Questions 2 to 7

```{r}

mrRobot <- function(hmm,simobs = 100, what = "acc"){

  # Simulation the robot simobs times 
  sim_rob <- simHMM(hmm,length = simobs)
  
  #extract the observations (x_t)
  observations <- sim_rob$observation
  
  #extract the hidden states z_t
  state_place <- sim_rob$states
  
### Filter 
    # Forward function computes the filtering with log
    log_filter <- forward(hmm = hmm, observation = observations)
    
    # Remove the log-transformation
    filter <- exp(log_filter)
    
    # Normalizing
    norm_filter <-prop.table(filter, margin = 2)
    
    # Checking which probability in each column is the highest
    most_prob_filter <- apply(norm_filter, MARGIN = 2, FUN = which.max)
    
    # Accuracy for the filter 
    accuracy_filter <-  sum(paste("z",most_prob_filter, sep = "") 
                         == state_place) / length(state_place)
  
    
    
### Smoothed (in this package called the posterior)
    smoothed <- posterior(hmm=hmm,observations)
    
    # Normalizing
    norm_smoothed <-prop.table(smoothed, margin = 2)
    
    most_prob_smoothed <- apply(norm_smoothed, MARGIN = 2, FUN = which.max)
    
    # Accuracy for the smoothed 
    accuracy_smoothed <-  sum(paste("z",most_prob_smoothed, sep = "") 
                              ==  state_place) / length(state_place)
  
  #cat("The accuracy of the smoothed is",accuracy_smoothed)
  
  
### Most probable path (Viterbi)
    mpp <- viterbi(hmm,observations)
    
    accuracy_mpp <-  sum(mpp == state_place) / length(state_place)
    #cat("The accuracy of the Viterbi is",accuracy_mpp)
  
  
    
    
  #Just logical statements on what to return. 
  if(what == "acc"){
  return( c(accuracy_filter = accuracy_filter, 
            accuracy_smoothed = accuracy_smoothed,
            accuracy_mpp = accuracy_mpp)) 
  }
  
  if(what == "filter"){
    return(norm_filter)
  }
  
  if(what == "smooth"){
    return(norm_smoothed)
  }
  
  if(what == "mpp"){
    return(mpp)
  }

}
```


## Filter

```{r, eval = FALSE}
mrRobot(hmm = robot,simobs = 100, what = "filter")
```
To much to print out, so run the code if you want to see the distribution

## Smoothed
```{r,eval = FALSE}
mrRobot(hmm = robot,simobs = 100, what = "smooth")

```
To much to print out, so run the code if you want to see the distribution

## Most probable path
```{r}
mrRobot(hmm = robot,simobs = 100, what = "viterbi")

```


```{r, cache = TRUE}
total_acc <- sapply(1:100,FUN = function(x){mrRobot(robot,100, what = "acc")} )

total_acc <- as.data.frame(t(total_acc))
total_acc$index <- 1:100 


ggplot(data = total_acc) + geom_line(aes(x=index,y=accuracy_filter , col = "Filter")) +  geom_line(aes(x=index,y=accuracy_smoothed , col = "Smoothed")) +  geom_line(aes(x=index,y=accuracy_mpp , col = "Mpp")) + theme_minimal() +xlab("samples") + ylab("acc of the sample")


```

```{r}
colMeans(total_acc[,-4])
```


## Question 6

```{r}

# filter_data <- sapply(1,FUN = function(x){
#   mrRobot(robot,100, what = "filter")
#   })

filter_data <- mrRobot(robot,100, what = "filter")
filter_entropy <- data.frame(Index = 1:100 ,
                  Entropy =
                  apply(filter_data, MARGIN = 2, FUN =entropy.empirical))

ggplot(data = filter_entropy, aes(x = Index, y = Entropy)) + geom_line() + ggtitle("Entropy for filter distribution") + theme_minimal()
```

No, the entropy remains random even while increasing the number of observations added to the hmm. This is because it is markovian and only depends on the previous observation.

## Question 7

```{r}

posterior <- filter_data[,100] # The last information of the robot aka the prior
transition <- robot$transProbs
transition %*% posterior 

```
Since we have a markovian assumption that the only relevant state is the one we are in now and since all previous states feeds forward through z100 and provides it with information about previout observations and states we can just multiply z100 probabilities for each state with the transition probabilities to ge at prediction of how z101 and s101 will be like. In this case we were asked to get the hidden states probabilites but if you want the observation you can just do a argmax over these probabilities to get were the next state should be.



## Forward Backward algorithm.

```{r}
trans_probs <- diag(1/2, 10) + 
  diag(1/2, 10)[, c(10, 1:9)]
emission_probs <- 
  diag(1/5, 10)[, c(3:10, 1:2)] +
  diag(1/5, 10)[, c(2:10, 1)] +
  diag(1/5, 10) + 
  diag(1/5, 10)[, c(10, 1:9)] + 
  diag(1/5, 10)[, c(9:10, 1:8)]

emission_density <- function(x, z) {
  return(emission_probs[z, x])
}

transition_density <- function(z, previous_z) {
  return(trans_probs[previous_z, z])
}

# transition_density2 <- function(z,previous_z){
#   if( z == zt){
#     
#     return(0.5)
#     
#   } else if( (z + 1) == zt){
#     return(0.5)
#   } else return(0)
# 
# }

get_alpha_scalar <- function(zt, xt, previous_alpha, previous_z) {
  # Args:
  #   zt Scalar, hidden state at which to compute alpha.
  #   xt Scalar, observed state.
  #   previous_alpha Vector, alpha for all z_{t-1}.
  #   previous_z     Vector, all z_{t-1}.
  
  summation_term <- 0
  for (i in 1:length(previous_z)) {
    summation_term <- summation_term +
      previous_alpha[i] * transition_density(zt, previous_z[i])
  }
  
  alpha <- emission_density(xt, zt) * sum(summation_term)
  return(alpha)
}

get_alpha <- function(Zt, xt, previous_alpha, previous_z) {
  # Args:
  #   Zt Vector, hidden states at which to compute alpha.
  #   xt Scalar, observed state.
  #   previous_alpha Vector, alpha for all z_{t-1}.
  #   previous_z     Vector, all z_{t-1}.  
  
  alpha <- sapply(Zt, function(zt) {
    get_alpha_scalar(zt, xt, previous_alpha, previous_z)
  })
  
  return(alpha)
}

get_beta_scalar <- function(zt, next_x, next_beta, next_z) {
  # Args:
  #   zt        Scalar, hidden state at which to compute alpha.
  #   next_x    Scalar, observed next state.
  #   next_beta Vector, alpha for all z_{t+1}.
  #   next_z    Vector, all z_{t+1}.
  
  summation_term <- 0
  for (i in 1:length(next_z)) {
    summation_term <- summation_term +
      next_beta[i] * emission_density(next_x, next_z[i]) * transition_density(next_z[i], zt)
  }
  
  # P(z_(t+1) | z_t) =
  # 0.5 if z_t = z_(t+1)
  # 0.5 if z_t = z_t + 1
  # 0 otherwise
  
  
  return(summation_term)
}

get_beta <- function(Zt, next_x, next_beta, next_z) {
  # Args:
  #   Zt        Vector, hidden states at which to compute alpha.
  #   next_x    Scalar, observed next state.
  #   next_beta Vector, alpha for all z_{t+1}.
  #   next_z    Vector, all z_{t+1}.  
  
  beta <- sapply(Zt, function(zt) {
    get_beta_scalar(zt, next_x, next_beta, next_z)
  })
  
  return(beta)
}

fb_algorithm <- function( 
  observations, 
  emission_density, 
  transition_density,
  possible_states,
  initial_density) {
  
  t_total <- length(observations)
  cardinality <- length(possible_states)
  
  # Alpha
  alpha <- matrix(NA, ncol=cardinality, nrow=t_total)
  
  for (i in 1:cardinality) {
    alpha[1, i] <- 
      emission_density(observations[1], possible_states[i]) * initial_density[i]
  }
  
  for (t in 2:t_total) {
    alpha[t, ] <- get_alpha(possible_states, observations[t], alpha[t - 1, ], possible_states)
  }
  
  
  # Beta
  beta <- matrix(NA, ncol=cardinality, nrow=t_total)
  
  beta[t_total, ] <- 1
  
  for (t in (t_total - 1):1) {
    beta[t, ] <- get_beta(possible_states, observations[t + 1], beta[t + 1, ], possible_states)
  }
  
  return(list(alpha = alpha, beta = beta))
}

filtering <- function(alpha) {
  alpha / rowSums(alpha)
}

smoothing <- function(alpha, beta) {
  alpha * beta / rowSums(alpha * beta)
}







robotHmm <- HMM::initHMM(
  States = 1:10,
  Symbols = 1:10,
  transProbs = trans_probs,
  emissionProbs = emission_probs
)

# Create a wrapper for simHMM to assign class to the output
simHMM <- function(hmm, length) {
  simulation <- HMM::simHMM(hmm, length)
  return(structure(simulation, class="HmmSimulation"))
}

# Simulate
nSim <- 100
robotSimultation <- simHMM(hmm=robotHmm, length=nSim)

#debugonce(fb_algorithm)

alphabeta <- fb_algorithm(observations = robotSimultation$observation, 
                          emission_density = emission_density, 
                          transition_density = transition_density,
                          possible_states = 1:10,
                          initial_density = rep(0.1, 10))

#filtering(alphabeta$alpha)
#smoothing(alphabeta$alpha, alphabeta$beta)


# plot(apply(filtering(alphabeta$alpha), 1, which.max), type = "l")
# plot(apply(smoothing(alphabeta$alpha, alphabeta$beta), 1, which.max), type = "l")
# lines(x = 1:100,robotSimultation$states, type = "l", col ="green")
# 



# # # Test
# zt <- 5
# xt <- 6
# previous_alpha <- rep(0.1, 10)
# previous_z <- 1:10
# transition_density(zt, previous_z[5])
# get_alpha_scalar(zt, xt, previous_alpha, previous_z)


# # Test
# zt <- 1:10
# xt <- 6
# previous_alpha <- rep(0.1, 10)
# previous_z <- 1:10
# transition_density(zt, previous_z[5])
# get_alpha(zt, xt, previous_alpha, previous_z)
#   


# # Test
# Zt <- 1:10
# next_x <- 6
# next_beta <- rep(0.1, 10)
# next_z <- 1:10
# transition_density(zt, previous_z[5])
# get_beta_scalar(5, next_x, next_beta, next_z)
# get_beta(zt, next_x, next_beta, next_z)



```

# Viterbi 

```{r}
# Define the transition, emission and initialization probabilities --------

emission_probs <- matrix(c(.2, .2, .2, 0, 0, 0, 0, 0, .2, .2,
                           .2, .2, .2, .2, 0, 0, 0, 0, 0, .2,
                           .2, .2, .2, .2, .2, 0, 0, 0, 0, 0,
                           0, .2, .2, .2, .2, .2, 0, 0, 0, 0,
                           0, 0, .2, .2, .2, .2, .2, 0, 0, 0,
                           0, 0, 0, .2, .2, .2, .2, .2, 0, 0,
                           0, 0, 0, 0, .2, .2, .2, .2, .2, 0,
                           0, 0, 0, 0, 0, .2, .2, .2, .2, .2,
                           .2, 0, 0, 0, 0, 0, .2, .2, .2, .2,
                           .2, .2, 0, 0, 0, 0, 0, .2, .2, .2), byrow=TRUE, nrow=10)

transition_probs <- matrix(c(.5, .5, 0, 0, 0, 0, 0, 0, 0, 0,
                             0, .5, .5, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, .5, .5, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, .5, .5, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, .5, .5, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, .5, .5, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, .5, .5, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, .5, .5, 0,
                             0, 0, 0, 0, 0, 0, 0, 0, .5, .5,
                             .5, 0, 0, 0, 0, 0, 0, 0, 0, .5), byrow=TRUE, nrow=10)

tProbDensity <- function(zt, zt_1) {
  return(transition_probs[zt_1, zt])
}

eProbDensity <- function(xt, zt) {
  return(emission_probs[zt, xt])
}

initProbDensity <- function(z0) {
  return(dunif(z0, min=1, max=10))
}


# Simulate data -----------------------------------------------------------

library(HMM)

robotHmm <- HMM::initHMM(
  States = 1:10,
  Symbols = 1:10,
  transProbs = transition_probs,
  emissionProbs = emission_probs
)

simHMM <- function(hmm, length) {
  simulation <- HMM::simHMM(hmm, length)
  return(structure(simulation, class="HmmSimulation"))
}

nSim <- 100
robotSimultation <- simHMM(hmm=robotHmm, length=nSim)

X <- robotSimultation$observation
Z <- robotSimultation$states

# Implement Viterbi -------------------------------------------------------

possibleStates <- 1:10
get_omega <- function(Z, Omega, Z_next, x_next) {
  sapply(Z_next, function(z_next) {
    term1 <- log(eProbDensity(x_next, z_next))
    
    term2 <- sapply(Z, function(z) {
      log(tProbDensity(z_next, z))
    }) + Omega
    
    return(term1+ max(term2))
  })
}

get_phi <- function(Z, Z_next, Omega) {
  sapply(Z_next, function(z_next) {
    term <- sapply(Z, function(z) {
      log(tProbDensity(z_next, z))
    }) + Omega
    return(Z[which.max(term)])
  }) 
}

viterbi <- function(observations, possibleStates) {
  cardinality <- length(possibleStates)
  t_total <- length(observations)
  
  omega_0 <- vector("numeric", length = cardinality)
  for (i in 1:cardinality) {
    omega_0[i] <- log(initProbDensity(possibleStates[i])) + 
      log(eProbDensity(observations[1], possibleStates[i]))
  }
  
  
  omega <- matrix(NA, nrow=t_total, ncol=cardinality)
  phi <- matrix(NA, nrow=t_total, ncol=cardinality)
  omega[1, ] <- omega_0
  
  for (i in 1:(t_total-1)) {
    omega[i+1, ] <- get_omega(possibleStates, omega[i, ], possibleStates, observations[i+1])
    phi[i+1, ] <- get_phi(possibleStates, possibleStates, omega[i, ])
  }
  
  mpp <- rep(NA, t_total)
  mpp[t_total] <- possibleStates[which.max(omega[t_total, ])]
  for (t in (t_total - 1):1) {
    mpp[t] <- phi[t + 1, possibleStates[mpp[t + 1]] == possibleStates]
  }
  
  return(list(path = mpp, omega = omega, phi = phi))
  
}

results <- viterbi(X, possibleStates)
results$path

results_HMM <- HMM::viterbi(robotHmm, X)

#cbind(results$path, results_HMM)

```


# Lab 3 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kernlab)
library(AtmRay)
library(ggplot2)
tullinge  <- read.csv('https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv', header=TRUE, sep=';')

data <- read.csv('https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/banknoteFraud.csv',
   header=FALSE, sep=',')

```

# Assignment 1

## a)

```{r}

# The kernel function 

exp_kern <- function(x,xi,l, sigmaf ){
  
  return((sigmaf^2)*exp(-0.5*( (x - xi) / l )^2))

}

# The implementation, can take a custom kernel of any class.

linear_gp <- function(x,y,xStar,hyperParam,sigmaNoise,kernel){

n <- length(x)  
kernel_f <- kernel 


# K = Covariance matrix calculation 
  K <- function(X, XI,...){ 
   
    kov <- matrix(0,nrow = length(X), ncol = length (XI)) 
    
    for(i in 1:length(XI)){
     
      kov[,i]<- kernel_f(X,XI[i],...) 
      
    }
    return(kov)
  }

l <-hyperParam[1]
sigmaf <- hyperParam[2]

#K(X,X)
K_xx   <- K(x,x, l = l, sigmaf = sigmaf) #, kernel = exp_kern

#K(X*,X*) 
K_xsxs <- K(xStar,xStar, l = l, sigmaf = sigmaf) # kernel = exp_kern,

#K(X,X*)
K_xxs  <- K(x,xStar, l = l, sigmaf = sigmaf)  #kernel = exp_kern,

# Algorithm in page 19 of the Rasmus/Williams book


sI <- sigmaNoise^2 * diag(dim(as.matrix(K_xx))[1])
# L is transposed according to a definition in the R & W book
L_transposed <- chol(K_xx + sI)
L <- t(L_transposed)
 
alpha <- solve(t(L), solve(L,y))

f_bar_star <- t(K_xxs) %*% alpha

v <- solve(L,K_xxs)

V_fs <- K_xsxs - t(v) %*% v   

log_mlike <- -0.5 %*% t(y) %*% alpha - sum( diag(L) - n/2 * log(2*pi) ) 

return(list(fbar = f_bar_star, vf = V_fs, log_post= log_mlike))

}




```




```{r, echo = FALSE}

# Utility function for the tasks

plot_gp<- function(plot_it,band_it){

ggplot() + 
  
  geom_point(
    aes(x = x, y = y), 
    data = plot_it,
    col = "blue",
    alpha = 0.7) + 
  
  geom_line(
    aes(x = xs, y = fbar),
    data = band_it, 
    alpha = 0.50) +
  
  geom_ribbon(
    aes(ymin = low, ymax = upp, xs), 
    data = band_it,
    alpha = 0.15) +
  
  theme_classic()
  
}


```




```{r}

# The data given
x <- c(-1.0, -0.6, -0.2, 0.4, 0.8)
y <- c(0.768, -0.044, -0.940, 0.719 ,-0.664)

# The noise
sn <- 0.1 

# The training grid
xs <- seq(-1,1,0.01)

# Hyperparameters l an sigma
hyperParam <- c(0.3, 1)

# Another utility function

repeter <- function(x,y,xs,sn,hyperParam,kernel){

res <- linear_gp(x,y,xs,hyperParam,sn,kernel)

# If you want the prediction band just add the noise variance (ie the sigma_n)

upp <- res$fbar + 1.96*sqrt(diag(res$vf))
low <- res$fbar - 1.96*sqrt(diag(res$vf))

plot_it <- data.frame(x = x, y = y)
band_it <- data.frame(xGrid = xs, fbar = res$fbar,  upp = upp, low = low)

plot_gp(plot_it,band_it)
}


```

## b)

```{r}
repeter(x = x[4], y = y[4],xs,sn,hyperParam, kernel = exp_kern)

```

## c)

```{r}
repeter(x = x[c(2,4)], y = y[c(2,4)],xs,sn,hyperParam, kernel = exp_kern)

```

## d)

```{r}
repeter(x = x, y = y,xs,sn,hyperParam, kernel = exp_kern)
```

## e)

```{r}
x <- c(-1.0, -0.6, -0.2, 0.4, 0.8)
y <- c(0.768, -0.044, -0.940, 0.719 ,-0.664)
sn <- 0.1 
xs <- seq(-1,1,0.01)
hyperParam <- c(1, 1)

repeter(x = x[4], y = y[4],xs,sn,hyperParam, kernel = exp_kern)
repeter(x = x[c(2,4)], y = y[c(2,4)],xs,sn,hyperParam, kernel = exp_kern)
repeter(x = x, y = y,xs,sn,hyperParam, kernel = exp_kern)

```


# Assignment 2 

## Data preparations 

```{r}
tullinge$time <- 1:nrow(tullinge)
tullinge$day <- rep(1:365,6)

time_sub <- tullinge$time %in% seq(1,2190,5)
tullinge <- tullinge[time_sub,]
```


## a) 

```{r}

kern_maker <- function(l,sigmaf){
  
  exp_k <- function(x,y = NULL){
    
   return((sigmaf^2)*exp(-0.5*( (x - y) / l )^2))
  }
 
  class(exp_k) <- "kernel"
  return(exp_k)
}



```


```{r}

# gausspr()
# kernelMatrix()


ell <- 1
# SEkernel <- rbfdot(sigma = 1/(2*ell^2)) # Note how I reparametrize the rbfdo (which is the SE kernel) in kernlab
# SEkernel(1,2) 

my_exp <- kern_maker(l = 10, sigmaf =20)

x <- c(1,3,4)
x_star <- c(2,3,4)
#my_exp(x,x_star)
kernelMatrix(my_exp,x,x_star)

```


## b) 

```{r}

lm_tull <- lm(temp ~ time + I(time^2), data = tullinge)

sigma_2n <- var(resid(lm_tull))


a2b_kern <- kern_maker(l = 0.2, sigmaf = 20 )
gp_tullinge <- gausspr(x = tullinge$time, 
                       y = tullinge$temp, 
                       kernel = a2b_kern,
                       var = sigma_2n)


```

See task c) for the plot. 

## c) 

```{r}

sn_2c <- sqrt(sigma_2n)
xs_2c <- tullinge$time
hyperParam_2c <- c(0.2, 20)


res_2c<- linear_gp(x = tullinge$time, 
        y = tullinge$temp,
        xStar = xs_2c,
        sigmaNoise  = sn_2c,
        hyperParam = hyperParam_2c,
        kernel = exp_kern)

upp2c <- predict(gp_tullinge) + 1.96*sqrt(diag(res_2c$vf))
low2c <- predict(gp_tullinge) - 1.96*sqrt(diag(res_2c$vf))

plot_it1 <- data.frame(x = tullinge$time, y = tullinge$temp)
band_it1 <- data.frame(xGrid = xs_2c, fbar = res_2c$fbar,  upp = upp2c, low = low2c)



```


```{r}
C2 <- ggplot() + 
  
   geom_point(
     aes(x = x, y = y), 
     data = plot_it1,
     col = "black",
     alpha = 0.7) + 
  
  geom_line(
    aes(x = xGrid, y = predict(gp_tullinge)),
    data = band_it1, 
    alpha = 1,
    col = "red") +
  
   geom_ribbon(
     aes(ymin = low2c, ymax = upp2c, x = xGrid), 
     data = band_it1,
     alpha = 0.2) + 
   
   theme_classic()

  #plot(x=band_it1$xGrid, y=band_it1$fbar, type = "l")

C2

```

## d) 


```{r}


a2d_kern <- kern_maker(l = 1.2, sigmaf = 20 )
gp_tullinge_d <- gausspr(x = tullinge$day,
                         y = tullinge$temp, 
                         kernel = a2d_kern,
                         var = sigma_2n)


C23 <- C2 + geom_line(aes(x= tullinge$time,y = predict(gp_tullinge_d)), col = "blue", size = 0.8) 
  #geom_point(data = tullinge, aes(x= time, y = temp)) + 
  
  
C23

# plot(y = tullinge$temp, x = tullinge$day)
# #lines(x = tullinge$time, y = fitted(lm_tull), col = "red")
# lines(x = tullinge$day, y = predict(gp_tullinge_d), col = "red" , lwd = 1)

```

The process model after time has an advantage in that sense that you can capture a trend isolated to a specific time since your modeling using the closest observations in time rather than the day model that assumes that the clostes related temperature point is the one on the same day previous years.

## e) 

```{r}




# periodic_kernel <- function(x,xi,sigmaf,d, l_1, l_2){
#  
#   part1 <- exp(2 * sin(pi * abs(x - xi) / d)^2 / l_1^2 )
#   part2 <- exp(-0.5 * abs(x - xi)^2 / l_2)
#   
#   sigmaf^2 * part1 * part2 
#   
# }



kern_maker2 <- function(sigmaf,d, l_1, l_2){
  
 periodic_kernel <- function(x,y = NULL){
 
  part1 <- exp(-2 * sin(pi * abs(x - y) / d)^2 / l_1^2 )
  part2 <- exp(-0.5 * abs(x - y)^2 / l_2^2)
  
  sigmaf^2 * part1 * part2 
  
}

 
  class(periodic_kernel) <- "kernel"
  return(periodic_kernel)
}


```


```{r}

sigmaff <- 20 
l1 <- 1 
l2 <- 10  
d_est <- 365 / sd(tullinge$time)
  
periodic_kernel <- kern_maker2(sigmaf = sigmaff,
                               d = d_est , 
                               l_1 = l1, 
                               l_2 = l2)

gp_tullinge_et <- gausspr(x = tullinge$time, 
                          y = tullinge$temp, 
                          kernel = periodic_kernel,
                          var = sigma_2n)


gp_tullinge_ed <- gausspr(x = tullinge$day, 
                          y = tullinge$temp, 
                          kernel = periodic_kernel,
                          var = sigma_2n)

```


```{r}
ggplot(data = tullinge, aes(x= time, y = temp)) +
  geom_point() + 
   geom_line(aes(y = predict(gp_tullinge_et)), col = "darkgreen", size = 0.8) 

```

This kernel seems to catch both variation in day and over time.

# Assignment 3

```{r}
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")
data[,5] <- as.factor(data[,5])
set.seed(111) 
SelectTraining <- sample(1:dim(data)[1], size = 1000, replace = FALSE)
train <- data[SelectTraining,]
test <- data[-SelectTraining,]

```


## a) 

```{r}
colnames(data)


GPfitFraud <- gausspr(fraud ~  varWave + skewWave, data = train)
GPfitFraud

# predict on the test set
fit_train<- predict(GPfitFraud,train[,c("varWave","skewWave")])
table(fit_train, train$fraud) # confusion matrix

mean(fit_train == train$fraud)

```


```{r}

# probPreds <- predict(GPfitIris, iris[,3:4], type="probabilities")
x1 <- seq(min(data[,"varWave"]),max(data[,"varWave"]),length=100)
x2 <- seq(min(data[,"skewWave"]),max(data[,"skewWave"]),length=100)
gridPoints <- meshgrid(x1, x2)
gridPoints <- cbind(c(gridPoints$x), c(gridPoints$y))

gridPoints <- data.frame(gridPoints)
names(gridPoints) <- c("varWave","skewWave")
probPreds <- predict(GPfitFraud, gridPoints, type="probabilities")

contour(x1,x2,t(matrix(probPreds[,1],100)), 20, 
        xlab = "varWave", ylab = "skewWave", 
        main = 'Prob(Fraud) - Fraud is red')

points(data[data[,5]== 1,"varWave"],data[data[,5]== 1,"skewWave"],col="blue")
points(data[data[,5]== 0,"varWave"],data[data[,5]== 0,"skewWave"],col="red")


```


## b) 


```{r}
# predict on the test set
fit_test<- predict(GPfitFraud,test[,c("varWave","skewWave")])
table(fit_test, test$fraud) # confusion matrix

mean(fit_test == test$fraud)
```

## c) 

```{r}


GPfitFraudFull <- gausspr(fraud ~ ., data = train)
GPfitFraudFull

# predict on the test set
fit_Full<- predict(GPfitFraudFull,test[,-ncol(test)])
table(fit_Full, test$fraud) # confusion matrix

mean(fit_Full == test$fraud)
```

# Lab 4
# The Implementation

```{r}
set.seed(12345)




sample_emission <- function(z, sd = 1){
  mean_adjust <- round(runif(1,-1,1))
  rnorm(1, mean = z + mean_adjust, sd = sd)
  
}

sample_transition <- function(z, sd = 1){
  mean_adjust <- round(runif(1,0,2))
  rnorm(1, mean = z + mean_adjust, sd = sd)
}

#Time <- 100

generate_data <- function(nobs, sd_emission = 1, sd_transition = 1){

X <- rep(NA,nobs)
Z_true <- c(runif(1,0,100),rep(NA,nobs-1))

X[1] <- sample_emission(Z_true[1], sd = sd_emission)

for (i in 2:nobs){
  
  Z_true[i] <- sample_transition(Z_true[i-1 ], sd_transition)
  X[i] <- sample_emission(Z_true[i], sd_emission)
}

return(list(X = X,
            Z_true = Z_true,
            sd_used = c(emission = sd_emission, transition = sd_transition)))
}


particle_filter <- function(nobs,X, sd_emission,sd_transition){
# Now we discard Z and try to estimate it using only X
Wt <- matrix(NA,ncol = nobs, nrow = nobs)
Z <- matrix(NA, ncol = nobs, nrow = nobs + 1)
Z[1,] <- runif(100, min = 0, max = 100)

for (t in 1:nobs){

# Emission model, the probability of the observation x_t given z_t 
emission <- sapply(Z[t,], function(zt) { 
     (dnorm(X[t],zt , sd_emission) + 
     dnorm(X[t],zt - 1, sd_emission) + 
     dnorm(X[t],zt + 1, sd_emission)) / 3 
})

Wt[t,] <- emission / sum(emission)


Z_old <- sample(x = Z[t,], size = nobs, replace = TRUE, prob = Wt[t,])

Z[t+1,] <- sapply(Z_old, function(x) sample_transition(x, sd  = sd_transition))

}

return(list(X = X,
            Z_est = Z[1:nobs,1:nobs],
            Wt = Wt)
)

}

# Calculate the weight Wt[t] for each particle
# Sample Z[t] particles with the weights
# Sample Z[t+1] with the sampled Z[t] particles

# Transition model, the probability of the hidden state z_{t+1} given z_t, the uncertainty of the observation z_t
```


# The special case 

```{r}

special_filter <- function(nobs,X, sd_emission,sd_transition){
# Now we discard Z and try to estimate it using only X
Wt <- matrix(NA,ncol = nobs, nrow = nobs)
Z <- matrix(NA, ncol = nobs, nrow = nobs + 1)
Z[1,] <- runif(100, min = 0, max = 100)

for (t in 1:nobs){

# Emission model, the probability of the observation x_t given z_t 
emission <- sapply(Z[t,], function(zt) { 
     (dnorm(X[t],zt , sd_emission) + 
     dnorm(X[t],zt - 1, sd_emission) + 
     dnorm(X[t],zt + 1, sd_emission)) / 3 
})

# All weights are equal in the special case.

Wt[t,] <- rep(1/nobs, nobs)


Z_old <- sample(x = Z[t,], size = nobs, replace = TRUE, prob = Wt[t,])

Z[t+1,] <- sapply(Z_old, function(x) sample_transition(x, sd  = sd_transition))

}

return(list(Z_est = Z[1:nobs,1:nobs],
            Wt = Wt)
)

}

```

## Running the functions

```{r}
gen_data_sd1 <- generate_data(nobs = 100, sd_emission = 1, sd_transition = 1)

est_data_sd1 <- particle_filter(nobs = 100, X = gen_data_sd1$X,
                sd_emission = 1, sd_transition = 1)

est_data_sd5 <- particle_filter(nobs = 100, X = gen_data_sd1$X,
                sd_emission = 5, sd_transition = 1)

est_data_sd50 <- particle_filter(nobs = 100, X = gen_data_sd1$X,
                sd_emission = 50, sd_transition = 1)

est_data_special <- special_filter(nobs = 100, X = gen_data_sd1$X,
                sd_emission = 50, sd_transition = 1)
```




```{r, warning = FALSE, message = FALSE}
library(rgdal)
library(rasterImage)
library(png)
library(animation)


r2d2 <- readPNG("r2d22.png")
#pman <- readPNG("pman.png")

#off <- 0.1

#Good advice, dont mess with the off parameter. 
plot_robot <- function(img,X,Z,Wt, off = 0.1){

# Some preparations before ploting
  
# Making the image to a raster object   
r2d2 <- as.raster(img) 

#Coordinates for the true position of the robot
C <- coordinates(data.frame(X = X, y = 0.01))

# The Expected position of the robot 
rm_z <- rowSums(Wt * Z)

  
for (i in 1:length(rm_z)){ 
plot(x = 1:250, y = rep(0,250),
     type = "l", 
     col = "white",
     ylim = c(-0.6,1), 
     xlab = "Position x",
     ylab = "", 
     main = paste("Time :",i))
  
abline(h = 0, col = "black") 
points(x = rm_z[i], y = 0.5, col = "white")
points(x = Z[i,], y = rep(-0.5,length(Z[i,]))) 

rasterImage(r2d2,
            xleft = C[i,1]-off ,
            ybottom = C[i,2]-off,
            xright = C[i,1]+off+50,
            ytop = C[i,2]+off+0.25,
            interpolate = FALSE)

points(x = C[i,1], y = -0.1, col = "green")
points(rm_z[i], y = -0.2, col = "red")

if(i == 1){
  Sys.sleep(5)
}

Sys.sleep(0.1)
}  

} 

```
\newpage

All plots are avaliable at my GitHub [LINK](https://github.com/Emil5KS/732A96/tree/master/Lab%204/Gifs) in form of gifs. 
With a standard deveation of 1 the expected value, represented by the red dot is close to the true position (the green dot) basicly from the begining of the observations.

For the standard deviation equal to 5 we have similar results as for sd of 1 but with a bit more uncertainty and a expected Z that is not always very close to the true state. 

For the standard deviation equal to 50 we have a harder time following the true location but it is still a good aproximation after a couple of itterations. 

For the special case with equal weights the predictions are useless, it is like running another independent robot and will continue doing that since we are not using the information from the emission model in our sampling.

\newpage

# Ploting Gifs 

```{r, eval = FALSE}
plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_sd1$Z,
           Wt = est_data_sd1$Wt,
           off = 0.1)

plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_sd5$Z,
           Wt = est_data_sd5$Wt,
           off = 0.1)

plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_sd50$Z,
           Wt = est_data_sd50$Wt,
           off = 0.1)

plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_special$Z,
           Wt = est_data_special$Wt,
           off = 0.1)
```

#Saving Gifs
```{r, eval = FALSE}
animation::saveGIF({
  plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_sd1$Z,
           Wt = est_data_sd1$Wt,
           off = 0.1)
},
movie.name = "robot_sd1.gif")

animation::saveGIF({
  plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_sd5$Z,
           Wt = est_data_sd5$Wt,
           off = 0.1)
},
movie.name = "robot_sd5.gif")

animation::saveGIF({
  plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_sd50$Z,
           Wt = est_data_sd50$Wt,
           off = 0.1)
},
movie.name = "robot_sd50.gif")

animation::saveGIF({
  plot_robot(img = r2d2, 
           X = gen_data_sd1$X, 
           Z = est_data_special$Z,
           Wt = est_data_special$Wt,
           off = 0.1)
},
movie.name = "robot_special.gif")

```

# A Kalman filter implementation

```{r, eval =  FALSE}

update_mu <- function(A, B=0, mu=0, mu_previous) {
  return(A%*%mu_previous)
}

update_sigma <- function(A, R, sigma_previous) {
  return(A%*%sigma_previous%*%t(A) + R)
}

calculate_kalman_gain <- function(sigma, C, Q) {
  sigma%*%t(C) %*% solve(C%*%sigma%*%t(C) + Q)
}

scale_mu_with_kalman_gain <- function(mu_bar, K, z, C) {
  mu_bar + K%*%(z-C%*%mu_bar)
}

scale_sigma_with_kalman_gain <- function(K, C, sigma_bar) {
  dimensions <- dim(K)
  I <- diag(1, nrow=dim(K)[1], ncol=dim(C)[2])
  
  return((I-K%*%C)%*%sigma_bar)
}

kalman_filter <- function(A, B, C, R, Q, mu_0, sigma_0, z) {
  n <- length(z)
  mu_list <- list()
  sigma_list <- list()
  
  mu_list[[1]] <- mu_0
  sigma_list[[1]] <- sigma_0
  
  for (t in 2:(n+1)) {
    mu_bar <- update_mu(A=A, mu_previous=mu_list[[t-1]])
    sigma_bar <- update_sigma(A=A, R=R, sigma_list[[t-1]])
    K <- calculate_kalman_gain(sigma=sigma_bar, C=C, Q=Q)
    mu_list[[t]] <- scale_mu_with_kalman_gain(mu_bar, K, z[t-1], C)
    sigma_list[[t]] <- scale_sigma_with_kalman_gain(K, C, sigma_bar)
  }
  return(structure(list(
    mu <- mu_list,
    sigma <- sigma_list
  )))
}


# Test
A <- matrix(c(1, 1, 0, 1), byrow=TRUE, nrow=2)
B <- matrix(c(0, 0, 0, 0), byrow=TRUE, nrow=2)
C <- matrix(c(1, 0), byrow=TRUE, nrow=1)
R <- matrix(c(0.035, 0.035+3.06*10^(-12), 3.06*10^(-12), 3.06*10^(-12)), byrow=TRUE, nrow=2)
Q <- 0.035
mu_0 <- matrix(c(10, 0), byrow=TRUE, nrow=2)
sigma_0 <- matrix(c(10^2, 0, 0, 10^2), byrow=TRUE, nrow=2)

load("../data/Radiation_data.Rda")
z <- Radiation_data$dose

debugonce(kalman_filter)
kalman_values <- kalman_filter(A, B, C, R, Q, mu_0, sigma_0, z)
kalman_mean <- unlist(lapply(kalman_values[[1]], function(x) x[[1]]))

plot(Radiation_data$dose, pch=16)
lines(kalman_mean, col="red")

```



